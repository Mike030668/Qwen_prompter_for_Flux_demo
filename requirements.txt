### Обновлённый `requirements.txt`

(ориентирован на **CUDA 12.1 GPU** - тот же стек, что мы довели до рабочего состояния)

```txt
# ───────────── Core (GPU, CUDA 12.1) ─────────────────────────────────────
torch==2.3.0+cu121            --extra-index-url https://download.pytorch.org/whl/cu121
torchvision==0.18.0+cu121     --extra-index-url https://download.pytorch.org/whl/cu121
triton==2.3.0                 --extra-index-url https://download.pytorch.org/whl/cu121
numpy==1.26.4                 # NumPy-1 ABI required by Torch 2.3 wheels

# минимальный набор драйверных библиотек (точно нужен libnccl / runtime)
nvidia-cublas-cu12==12.1.3.1
nvidia-cuda-runtime-cu12==12.1.105
nvidia-cuda-cupti-cu12==12.1.105
nvidia-nccl-cu12==2.20.5
nvidia-nvtx-cu12==12.1.105

# ───────────── Prompt-side (Qwen & helpers) ──────────────────────────────
transformers==4.53.0
sentencepiece>=0.2.0
auto-gptq==0.7.1                    # GPTQ quantised weights loader
peft==0.10.0                        # LoRA / QLoRA utilities

# accelerate ≥ 0.29 дает параметры force_hooks+strict;  
# ставим **без авто-deps**, чтобы не обновить torch.
accelerate==0.29.3   --no-deps

huggingface-hub>=0.23.0             # модельные загрузки / auth
protobuf>=3.20,<5                   # требуется T5-конвертеру токенизатора

# ───────────── Diffusion / Generation (Flux) ─────────────────────────────
diffusers==0.30.0
safetensors>=0.4.2

# опционально: memory-efficient attention; wheel под Torch 2.3 готов
xformers==0.0.25.post1 ; sys_platform == "linux"

# ───────────── Misc. ─────────────────────────────────────────────────────
tqdm>=4.66
Pillow>=11.3.0

# ───────────── Core ────────────────────────────────────────────────────────
torch==2.3.0+cu121   --extra-index-url https
```

> **Установка с `uv pip`:**
>
> ```bash
> uv pip install -r requirements.txt  # ускоренная загрузка
> # затем отдельно, БЕЗ зависимостей, accelerate:
> uv pip install --no-deps accelerate==0.29.3
> ```
>
> Параметр `--no-deps` гарантирует, что `uv` не попытается обновить `torch`
> и связанные `nvidia-*` колёса.

### Примечания

* Для видеокарт ≤ 8 GB VRAM используйте модель
  `FLUX_ID = "Freepik/flux.1-lite-8B-alpha"` **и** создавайте пайп-лайн так:

  ```python
  pipe = FluxPipeline.from_pretrained(
      FLUX_ID,
      torch_dtype=torch.float16,
      device_map="auto",                        # CPU-offload
      scheduler=DPMSolverMultistepScheduler.from_pretrained(
          FLUX_ID, subfolder="scheduler"
      ),
  )
  ```

* Патч-заглушка, который добавляет `accelerate.utils.memory.clear_device_cache`,
  уже встроен в `qwen_prompter.py` и `demo_flux.py`; оставьте его.

* Если понадобится собрать xFormers вручную:

  ```bash
  uv pip uninstall xformers
  UV_PIP_NO_BINARY=:all: uv pip install --no-deps \
    git+https://github.com/facebookresearch/xformers.git@v0.0.25 --verbose
  ```

