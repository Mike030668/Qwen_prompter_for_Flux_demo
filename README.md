
# ranger_generation

End-to-end pipeline:

1. **Qwen-Prompter** üëâ generates structured JSON (positive / negative / params)
2. **Flux 1** üëâ text-to-image (multiple seeds, dynamic batching)
3. **SigLIP-2** üëâ scores & ranks images (coming next)

---

## 1. Quick install (CUDA 12.1 GPU)

```bash
python -m venv .venv && source .venv/bin/activate          # or use conda/mamba
uv pip install --upgrade pip                                # optional but fast
uv pip install -r requirements.txt
````

> ‚Ä¢ For inference on CPU only, drop the `+cu121` wheels and install plain
> `torch==2.3.0 torchvision==0.18.0` (much slower).
> ‚Ä¢ Tested on Ubuntu 22.04, Py 3.10.18, RTX 4090 + driver 576, CUDA 12.9.

---

## 2. Hugging Face authentication

Flux checkpoints are gated.
Create a **read** token on [https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens) and run:

```bash
huggingface-cli login
# or
export HUGGINGFACE_HUB_TOKEN=hf_********************************
```

---

## 3. First-run demo

```bash
python scripts/demo_prompt.py "–•–æ—á—É –°–±–µ—Ä–∫–æ—Ç–∞ –∞–Ω—Ñ–∞—Å..."
python scripts/demo_flux.py   "–•–æ—á—É –°–±–µ—Ä–∫–æ—Ç–∞ –∞–Ω—Ñ–∞—Å..."
xdg-open samples.png          # (or open in your OS)
```

The first call downloads ‚âà 10 GB of Flux weights; afterwards each run is instant.

---

## 4. Using LoRA / QLoRA adapters with Flux 1

```bash
# ‚ë† Place adapter folder inside ./checkpoints or any path
export LORA_PATH=/path/to/my_cinematic_lora

# ‚ë° Enable LoRA loading in flux_runner.py
from diffusers.loaders import FluxLoraLoaderMixin
pipe = FluxPipeline.from_pretrained(
    FLUX_ID,
    torch_dtype=torch.float16,
    lora_loader=FluxLoraLoaderMixin.from_pretrained(LORA_PATH),
    ...
)
```

*If you need to **train** a new adapter:*

```bash
# example: DreamBooth-style fine-tune
accelerate launch train_lora.py \
  --pretrained_model_name_or_path black-forest-labs/FLUX.1-schnell \
  --instance_data_dir ./my_dataset --output_dir ./my_lora \
  --resolution 1024 --train_text_encoder --mixed_precision fp16
```

---

## 5. Known issues & patches

| Symptom                               | Fix                                                                                                                                                     |
| ------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `ImportError: clear_device_cache`     | Patched at runtime: the first lines of `ranger_generation/prompter/qwen_prompter.py` add a stub to `accelerate.utils.memory` for any accelerate ‚â• 0.25. |
| `xFormers can't load C++/CUDA`        | Rebuild xformers against your Torch (see above) *or* ignore (pipeline works without memory-efficient attention).                                        |
| AutoGPTQ ‚ÄúCUDA kernels not installed‚Äù | Optional speed-up; compile with `python -m auto_gptq.cuda_setup install` if desired.                                                                    |

---

*Happy generating!*

```


## 6 –ö–∞–∫ –ø–æ–¥–∫–ª—é—á–∏—Ç—å –≤–∞—à LoRA-–∞–¥–∞–ø—Ç–µ—Ä
–ü–æ–¥–≥–æ—Ç–æ–≤—å—Ç–µ –∏–ª–∏ –æ–±—É—á–∏—Ç–µ LoRA-–≤–µ—Å–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, —á–µ—Ä–µ–∑ ai-toolkit –∏ train_lora_flux_24gb.yaml).

–ü–æ–ª–æ–∂–∏—Ç–µ my_lora.safetensors –∫—É–¥–∞ —É–≥–æ–¥–Ω–æ –Ω–∞ –¥–∏—Å–∫.

–ü–µ—Ä–µ–¥ –∑–∞–ø—É—Å–∫–æ–º –¥–µ–º–æ-—Å–∫—Ä–∏–ø—Ç–∞ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –æ–∫—Ä—É–∂–µ–Ω–∏—è:

bash
–ö–æ–ø–∏—Ä–æ–≤–∞—Ç—å
–†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å
export FLUX_LORA_PATH=/–ø—É—Ç—å/–∫/my_lora.safetensors
–ó–∞–ø—É—Å–∫–∞–π—Ç–µ:

bash
–ö–æ–ø–∏—Ä–æ–≤–∞—Ç—å
–†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å
python scripts/demo_flux.py "–í–∞—à –∑–∞–ø—Ä–æ—Å" \
  --num_images 4 --width 512 --height 512 --steps 15 --scale 7.5 \
  --output my_out.png
‚Äî –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ LoRA-–∞–¥–∞–ø—Ç–µ—Ä –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∑–∞–≥—Ä—É–∑–∏—Ç—Å—è –≤ FluxPipeline.


---

### Next steps

* When SigLIP-2 ranking is ready, add its runtime dependency (`siglip2` or custom scorer) and update the README.  
* Consider wrapping `scripts/demo_flux.py` into a FastAPI endpoint for easy integration.

Let me know if you‚Äôd like any tweaks!
```
